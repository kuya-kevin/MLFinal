{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8685878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dcd64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_K8.csv\", header = None, low_memory = False)  # process the data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f733920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffc559e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5399</th>\n",
       "      <th>5400</th>\n",
       "      <th>5401</th>\n",
       "      <th>5402</th>\n",
       "      <th>5403</th>\n",
       "      <th>5404</th>\n",
       "      <th>5405</th>\n",
       "      <th>5406</th>\n",
       "      <th>5407</th>\n",
       "      <th>5408</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5409 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1      2      3      4      5      6      7      8      9     ...  \\\n",
       "0 -0.161 -0.014  0.002 -0.036 -0.033 -0.093  0.025  0.005  0.000 -0.015  ...   \n",
       "1 -0.158 -0.002 -0.012 -0.025 -0.012 -0.106  0.013  0.005  0.000 -0.002  ...   \n",
       "2 -0.169 -0.025 -0.010 -0.041 -0.045 -0.069  0.038  0.014  0.008 -0.014  ...   \n",
       "3 -0.183 -0.051 -0.023 -0.077 -0.092 -0.015  0.071  0.027  0.020 -0.019  ...   \n",
       "4 -0.154  0.005 -0.011 -0.013 -0.002 -0.115  0.005  0.002 -0.003  0.002  ...   \n",
       "\n",
       "    5399   5400   5401   5402   5403   5404   5405   5406   5407  5408  \n",
       "0  0.006  0.013  0.021  0.020  0.016 -0.011  0.003  0.010 -0.007     0  \n",
       "1  0.002 -0.008  0.007  0.015 -0.008 -0.011 -0.004  0.013  0.005     0  \n",
       "2  0.019  0.010  0.025  0.025  0.021 -0.012  0.006  0.016 -0.018     0  \n",
       "3  0.051  0.012  0.050  0.038  0.051 -0.015  0.017  0.027 -0.049     0  \n",
       "4 -0.011  0.012  0.009  0.003 -0.001  0.002 -0.006  0.009  0.013     0  \n",
       "\n",
       "[5 rows x 5409 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afe6bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the X and y from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "882e84f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of X (16592, 5408)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [x for x in range(5408)]\n",
    "X = df[feature_cols]\n",
    "y = df[5408]\n",
    "print(\"dimensions of X\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e01da8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on X, finding PCs that explains \"percent\" of data\n",
    "# Output: X_reduced, X with reduced dimension\n",
    "def PCA_X(X, percent):\n",
    "    feature_cols = [x for x in range(5408)]     # store the features by their indexes\n",
    "    feature_cols_np = np.array(feature_cols)\n",
    "\n",
    "    mean_center_X = X - np.mean(X, axis = 0)    # get the mean centered X from X\n",
    "\n",
    "    # calculate the covar_matrix\n",
    "    covar_matrix = mean_center_X.T @ mean_center_X / (len(mean_center_X) - 1)\n",
    "\n",
    "    # perform eigendecomposition, getting eig_val and eig_vector\n",
    "    eig_val, eig_vector = eig(covar_matrix)\n",
    "    print(\"eig_val\", eig_val)\n",
    "    print(\"eig_vector\", eig_vector)\n",
    "    print(\"len eig_val\", len(eig_val))\n",
    "    print(\"len eig_vector\", len(eig_vector))\n",
    "\n",
    "    # sort through eigen_val, creating \"indexes\"\n",
    "    sorted_indexes = eig_val.argsort()[::-1][:len(eig_val)]\n",
    "    print(\"sorted_indexes\", sorted_indexes)\n",
    "    eig_val = eig_val[sorted_indexes]\n",
    "\n",
    "    eig_vector = eig_vector[:,sorted_indexes]   # sort the eig_vector based on sorted_indexes\n",
    "    feature_cols_np = feature_cols_np[sorted_indexes] # sort the feature_cols based on sorted_indexes\n",
    "\n",
    "    sum_eig = sum(eig_val)                      # sum over all eig_val for determining percent of variability\n",
    "\n",
    "    # up toward what number of principle components does 95% of data's variability get explained\n",
    "    count = 0\n",
    "    sum_eig_sofar = 0\n",
    "    for i in range(len(eig_val)):\n",
    "        if sum_eig_sofar < (percent * sum_eig):\n",
    "            sum_eig_sofar += eig_val[i]\n",
    "            count += 1\n",
    "\n",
    "    print(\"count\", count)\n",
    "    print(\"feature_cols_np\", feature_cols_np)\n",
    "    #for i in range(count):\n",
    "    #    print(feature_cols_np[i])\n",
    "\n",
    "    # get eig_vectors that explains \"percent\" of data\n",
    "    eig_vector_reduced = eig_vector[:, 0:count]\n",
    "    \n",
    "    # get X_reduced by projecting each data point in X to the M dimensions described by M eigenvectors\n",
    "    # Note: M here is the amount of eigenvectors that explains \"percent\" of data\n",
    "    X_reduced = mean_center_X @ eig_vector_reduced\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34dbe206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eig_val [2.26180309e+04 7.15492481e+03 4.84945490e+03 ... 5.51324826e-07\n",
      " 6.35419047e-07 6.24391237e-07]\n",
      "eig_vector [[ 8.54825847e-05 -5.04448186e-05 -1.56810934e-04 ... -2.56352334e-04\n",
      "   2.14088887e-05  1.47862667e-04]\n",
      " [ 5.04186112e-05 -1.24204596e-04 -2.10201415e-04 ... -3.00425200e-04\n",
      "   7.32882954e-04  3.15934092e-04]\n",
      " [ 2.57219310e-05  5.44578611e-05  3.39219570e-05 ... -1.35169558e-03\n",
      "  -3.04687593e-03 -2.77420575e-04]\n",
      " ...\n",
      " [-6.85638827e-06  7.28126254e-05  8.82171058e-05 ... -2.20932467e-02\n",
      "  -9.11774535e-02 -4.26462957e-02]\n",
      " [-3.26821800e-06  8.61183584e-06  1.79260291e-05 ... -2.71030260e-02\n",
      "  -7.54592015e-02 -2.65584967e-02]\n",
      " [ 1.92276476e-05 -1.58473137e-04 -1.95015608e-04 ...  6.91038245e-02\n",
      "   4.27699587e-02  9.76849976e-02]]\n",
      "len eig_val 5408\n",
      "len eig_vector 5408\n",
      "sorted_indexes [   0    1    2 ... 4919 4897 4916]\n",
      "count 449\n",
      "feature_cols_np [   0    1    2 ... 4919 4897 4916]\n",
      "X_reduced               0           1           2          3          4          5    \\\n",
      "0      -99.034409   -7.419601   40.188607  24.842250 -14.403953  21.816217   \n",
      "1      -94.147791  -56.094259  -16.929008  17.272232   2.979882   7.822706   \n",
      "2     -102.587280   39.656470   51.129907  29.499727  16.052961 -11.690440   \n",
      "3     -121.168330   62.841733   69.024195  21.117324  25.251788 -13.546003   \n",
      "4      -87.347339  -26.984050  -70.057058  15.659031  29.432183  13.047042   \n",
      "...           ...         ...         ...        ...        ...        ...   \n",
      "16587 -103.244033   83.351473   86.922323  31.799299  30.526576 -32.379339   \n",
      "16588  -95.114158  -57.904012  -25.667544  17.332584   4.469778   6.628930   \n",
      "16589 -129.170000  122.897234  103.618256  39.228568  32.866488 -78.011065   \n",
      "16590 -115.854073  126.245054   94.362306  33.224817  29.371590 -37.112579   \n",
      "16591  -95.150071  -53.078001  -17.239730  17.921438   0.190722  14.804273   \n",
      "\n",
      "             6          7          8          9    ...       439       440  \\\n",
      "0     -15.134985  -1.920683  29.507110   5.795828  ... -2.577051  1.436868   \n",
      "1     -11.567577  10.894727 -16.630890   1.006128  ... -2.098982  0.092675   \n",
      "2     -14.225221  11.777391 -15.325525  36.676883  ... -1.069914  0.237261   \n",
      "3      21.214188   3.443880  34.930914   7.663473  ...  2.631610 -1.558147   \n",
      "4     -43.374933  12.280241 -34.132090  -0.296163  ... -1.141316  0.352370   \n",
      "...          ...        ...        ...        ...  ...       ...       ...   \n",
      "16587   8.692778  -3.819703   0.010586  23.003784  ... -1.283796 -1.401696   \n",
      "16588  -7.412482  10.973832 -23.375480   3.960074  ...  1.007850 -1.119172   \n",
      "16589   3.470681   1.491682 -17.255288 -32.191895  ... -1.543204 -0.099567   \n",
      "16590  38.949057  14.487051   7.266258  -0.818876  ... -3.508642  3.157073   \n",
      "16591 -14.208251   9.355833 -21.556947   3.845205  ...  0.979529 -1.109905   \n",
      "\n",
      "            441       442       443       444       445       446       447  \\\n",
      "0     -0.121418 -3.088479 -1.493998 -2.865834  1.085939  0.655942 -1.514548   \n",
      "1      0.593944 -2.691238 -0.534951 -2.742397  1.551924  0.650094 -2.711023   \n",
      "2     -0.437744 -0.424932  0.662443 -0.164814  0.125205  0.682685 -0.530485   \n",
      "3      1.420909 -2.592339  0.696102  0.834533 -0.621136  1.957509  1.420083   \n",
      "4      1.624572 -4.319472 -3.732382  2.329987 -2.793347 -0.413837 -0.484591   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "16587  0.711608  0.878838  0.745426  1.508495 -0.679820  1.543885 -0.834526   \n",
      "16588  0.404554  0.426356  0.659919  0.477925 -1.079211 -0.220764 -1.017474   \n",
      "16589 -0.707664 -0.210431  1.597455 -0.950756 -0.417647  1.276628 -0.698460   \n",
      "16590  2.021897 -3.369044 -1.006642 -4.239079 -0.272996  1.630950 -1.490765   \n",
      "16591  0.738903  0.210785  0.218669  1.033933 -0.762718  1.065593 -0.011271   \n",
      "\n",
      "            448  \n",
      "0      1.165805  \n",
      "1      0.282311  \n",
      "2      0.581548  \n",
      "3      0.693517  \n",
      "4      0.566704  \n",
      "...         ...  \n",
      "16587  0.171458  \n",
      "16588 -0.432744  \n",
      "16589  1.585968  \n",
      "16590  1.049873  \n",
      "16591 -0.976784  \n",
      "\n",
      "[16592 rows x 449 columns]\n",
      "X_reduced.shape (16592, 449)\n",
      "X.shape (16592, 5408)\n"
     ]
    }
   ],
   "source": [
    "X_reduced = PCA_X(X, 0.99)\n",
    "print(\"X_reduced\", X_reduced)\n",
    "print(\"X_reduced.shape\", X_reduced.shape)\n",
    "print(\"X.shape\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c6925",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d7baf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learn_rate = 0.001, num_iters=1000):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.W = None \n",
    "        self.bias = None\n",
    "    \n",
    "    # X is num_samples by num_features \n",
    "    # y is 1D row vector for each training sample\n",
    "    def fit(self, X, y):\n",
    "        # init params (as zeros)\n",
    "        num_samples, num_features = X.shape\n",
    "        self.W = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        #print(\"num_samples, num_features\", num_samples, num_features)\n",
    "        #print(\"self.W.shape\", self.W.shape)\n",
    "        \n",
    "        # gradient descent\n",
    "        for i in range(self.num_iters):\n",
    "            linear_model = np.dot(X, self.W) + self.bias \n",
    "            \n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "            \n",
    "            # derivatives\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # update weights and bias \n",
    "            self.W -= self.learn_rate * dw\n",
    "            self.bias -= self.learn_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.W) + self.bias \n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        \n",
    "        # based on y_predicted, get the predicted class label\n",
    "        y_predicted_label = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        \n",
    "        return y_predicted_label\n",
    "    \n",
    "    # sigmoid func\n",
    "    def _sigmoid(self, x):\n",
    "        sigmoid = 1 / (1 + np.exp(-x))\n",
    "        return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "181ca521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate accuracy \n",
    "def accuracy(y_observed, y_predicted):\n",
    "    accuracy = np.sum(y_observed == y_predicted) / len(y_observed)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b35519d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_calc(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 1234)\n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    Logistic_regressor = LogisticRegression(learn_rate = 0.001, num_iters=1000)\n",
    "    Logistic_regressor.fit(X_train, y_train)\n",
    "    predictions = Logistic_regressor.predict(X_test)\n",
    "    \n",
    "    print(\"Logistic classification accurary:\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4c38283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (14103, 5408)\n",
      "Logistic classification accurary: 0.991562876657292\n"
     ]
    }
   ],
   "source": [
    "# logistic regression on original data \n",
    "LogisticRegression_calc(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ab074e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (14103, 449)\n",
      "Logistic classification accurary: 0.6593009240658899\n"
     ]
    }
   ],
   "source": [
    "# logistic regression on dimensionally reduced data\n",
    "LogisticRegression_calc(X_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
