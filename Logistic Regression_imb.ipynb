{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d609d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file runs logistic regression with original imbalanced data \"cleaned_K8.csv\"\n",
    "# It first performs PCA to check how many principle components are needed for 90% and 95% of the variability of the data\n",
    "# It runs the logistic regression implemented by us and by sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8685878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcd64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned_K8.csv\", header = None, low_memory = False)  # process the data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f733920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc559e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5399</th>\n",
       "      <th>5400</th>\n",
       "      <th>5401</th>\n",
       "      <th>5402</th>\n",
       "      <th>5403</th>\n",
       "      <th>5404</th>\n",
       "      <th>5405</th>\n",
       "      <th>5406</th>\n",
       "      <th>5407</th>\n",
       "      <th>5408</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5409 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1      2      3      4      5      6      7      8      9     ...  \\\n",
       "0 -0.161 -0.014  0.002 -0.036 -0.033 -0.093  0.025  0.005  0.000 -0.015  ...   \n",
       "1 -0.158 -0.002 -0.012 -0.025 -0.012 -0.106  0.013  0.005  0.000 -0.002  ...   \n",
       "2 -0.169 -0.025 -0.010 -0.041 -0.045 -0.069  0.038  0.014  0.008 -0.014  ...   \n",
       "3 -0.183 -0.051 -0.023 -0.077 -0.092 -0.015  0.071  0.027  0.020 -0.019  ...   \n",
       "4 -0.154  0.005 -0.011 -0.013 -0.002 -0.115  0.005  0.002 -0.003  0.002  ...   \n",
       "\n",
       "    5399   5400   5401   5402   5403   5404   5405   5406   5407  5408  \n",
       "0  0.006  0.013  0.021  0.020  0.016 -0.011  0.003  0.010 -0.007     0  \n",
       "1  0.002 -0.008  0.007  0.015 -0.008 -0.011 -0.004  0.013  0.005     0  \n",
       "2  0.019  0.010  0.025  0.025  0.021 -0.012  0.006  0.016 -0.018     0  \n",
       "3  0.051  0.012  0.050  0.038  0.051 -0.015  0.017  0.027 -0.049     0  \n",
       "4 -0.011  0.012  0.009  0.003 -0.001  0.002 -0.006  0.009  0.013     0  \n",
       "\n",
       "[5 rows x 5409 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe6bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the X and y from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882e84f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of X (16592, 5408)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [x for x in range(5408)]\n",
    "X = df[feature_cols]\n",
    "y = df[5408]\n",
    "print(\"dimensions of X\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e01da8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on X, finding PCs that explains \"percent\" of data\n",
    "# Output: X_reduced, X with reduced dimension\n",
    "def PCA_X(X, percent):\n",
    "    feature_cols = [x for x in range(5408)]     # store the features by their indexes\n",
    "    feature_cols_np = np.array(feature_cols)\n",
    "\n",
    "    mean_center_X = X - np.mean(X, axis = 0)    # get the mean centered X from X\n",
    "\n",
    "    # calculate the covar_matrix\n",
    "    covar_matrix = mean_center_X.T @ mean_center_X / (len(mean_center_X) - 1)\n",
    "\n",
    "    # perform eigendecomposition, getting eig_val and eig_vector\n",
    "    eig_val, eig_vector = eig(covar_matrix)\n",
    "    \n",
    "\n",
    "    # sort through eigen_val, creating \"indexes\"\n",
    "    sorted_indexes = eig_val.argsort()[::-1][:len(eig_val)]\n",
    "    eig_val = eig_val[sorted_indexes]\n",
    "\n",
    "    eig_vector = eig_vector[:,sorted_indexes]   # sort the eig_vector based on sorted_indexes\n",
    "    feature_cols_np = feature_cols_np[sorted_indexes] # sort the feature_cols based on sorted_indexes\n",
    "\n",
    "    sum_eig = sum(eig_val)                      # sum over all eig_val for determining percent of variability\n",
    "\n",
    "    # up toward what number of principle components does \"percent\" of data's variability get explained\n",
    "    count = 0\n",
    "    sum_eig_sofar = 0\n",
    "    for i in range(len(eig_val)):\n",
    "        if sum_eig_sofar < (percent * sum_eig):\n",
    "            sum_eig_sofar += eig_val[i]\n",
    "            count += 1\n",
    "\n",
    "    print(\"Amount of principle components that explains\", percent*100, \"%:\", count)\n",
    "\n",
    "    # get eig_vectors that explains \"percent\" of data\n",
    "    eig_vector_reduced = eig_vector[:, 0:count]\n",
    "    \n",
    "    # get X_reduced by projecting each data point in X to the M dimensions described by M eigenvectors\n",
    "    # Note: M here is the amount of eigenvectors that explains \"percent\" of data\n",
    "    X_reduced = mean_center_X @ eig_vector_reduced\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34dbe206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of principle components that explains 90.0 %: 48\n",
      "Amount of principle components that explains 95.0 %: 127\n"
     ]
    }
   ],
   "source": [
    "X_reduced = PCA_X(X, 0.90)\n",
    "X_reduced = PCA_X(X, 0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1790af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionHand:\n",
    "    def __init__(self, learn_rate = 0.001, num_iters=10000):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.W = None \n",
    "        self.bias = None\n",
    "    \n",
    "    # X is num_samples by num_features \n",
    "    # y is 1D row vector for each training sample\n",
    "    def fit(self, X, y):\n",
    "        # init params (as zeros)\n",
    "        num_samples, num_features = X.shape\n",
    "        self.W = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        #print(\"num_samples, num_features\", num_samples, num_features)\n",
    "        #print(\"self.W.shape\", self.W.shape)\n",
    "        \n",
    "        # gradient descent\n",
    "        for i in range(self.num_iters):\n",
    "            linear_model = np.dot(X, self.W) + self.bias \n",
    "            \n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "            \n",
    "            # derivatives\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # update weights and bias \n",
    "            self.W -= self.learn_rate * dw\n",
    "            self.bias -= self.learn_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.W) + self.bias \n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        \n",
    "        # based on y_predicted, get the predicted class label\n",
    "        y_predicted_label = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        \n",
    "        return y_predicted_label\n",
    "    \n",
    "    # sigmoid func\n",
    "    def _sigmoid(self, x):\n",
    "        sigmoid = 1 / (1 + np.exp(-x))\n",
    "        return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "181ca521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate accuracy \n",
    "def accuracy(y_observed, y_predicted):\n",
    "    accuracy = np.sum(y_observed == y_predicted) / len(y_observed)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47654154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_package(X, y, testSize):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = 1234)\n",
    "    clf = LogisticRegression(max_iter = 10000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_predictions = clf.predict(X_test)\n",
    "    print(\"\")\n",
    "    print(\"For SMOTE + RandomUnderSampler with LR from sklearn package:\")\n",
    "    print(\"Logistic classification accurary:\", accuracy(y_test, y_predictions))\n",
    "    print(\"precision_score\", precision_score(y_test, y_predictions))\n",
    "    print(\"recall_score\", recall_score(y_test, y_predictions))\n",
    "    print(\"f1_score\", f1_score(y_test, y_predictions))\n",
    "    print(\"roc_auc_score\", roc_auc_score(y_test, y_predictions))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_predictions)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b35519d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter, parameter: testSize, \n",
    "def LogisticRegression_calc(X, y, testSize):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = 1234)\n",
    "    \n",
    "    Logistic_regressor = LogisticRegressionHand(learn_rate = 0.001, num_iters=10000)\n",
    "    Logistic_regressor.fit(X_train, y_train)\n",
    "    y_predictions = Logistic_regressor.predict(X_test)\n",
    "    print(\"\")\n",
    "    print(\"For SMOTE + RandomUnderSampler with hand-implemented LR:\")\n",
    "    print(\"Logistic classification accurary:\", accuracy(y_test, y_predictions))\n",
    "    print(\"precision_score\", precision_score(y_test, y_predictions))\n",
    "    print(\"recall_score\", recall_score(y_test, y_predictions))\n",
    "    print(\"f1_score\", f1_score(y_test, y_predictions))\n",
    "    print(\"roc_auc_score\", roc_auc_score(y_test, y_predictions))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_predictions)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4c38283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For SMOTE + RandomUnderSampler with hand-implemented LR:\n",
      "Logistic classification accurary: 0.9929690638810768\n",
      "precision_score 0.7894736842105263\n",
      "recall_score 0.32608695652173914\n",
      "f1_score 0.46153846153846156\n",
      "roc_auc_score 0.6626379632568145\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logistic regression on original data \n",
    "LogisticRegression_calc(X, y, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ab074e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For SMOTE + RandomUnderSampler with LR from sklearn package:\n",
      "Logistic classification accurary: 0.9889513860988349\n",
      "precision_score 0.42105263157894735\n",
      "recall_score 0.5217391304347826\n",
      "f1_score 0.46601941747572817\n",
      "roc_auc_score 0.7575240664339363\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use sklearn package \n",
    "LogisticRegression_package(X, y, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf106d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
